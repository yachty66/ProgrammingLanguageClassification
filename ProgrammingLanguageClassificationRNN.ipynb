{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efbc684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build using LSTM\n",
    "#LSTM are used for sequential data\n",
    "#sequential data = order of data matters\n",
    "#use this tutorial https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "'''\n",
    "Problem:\n",
    "    I dont know how to create a vocab but if i can solve this problem i can solve also the other problems.\n",
    "    just need to copy steps from standard thing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2117b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab547667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14026</td>\n",
       "      <td>var result = testObj1 | testObj2;\\...</td>\n",
       "      <td>c-sharp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12201</td>\n",
       "      <td>///     Initializes a new instance of ...</td>\n",
       "      <td>c-sharp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17074</td>\n",
       "      <td>/*\\n\\n     Explanation :- a user gives a Strin...</td>\n",
       "      <td>javascript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21102</td>\n",
       "      <td>int sum = 0;\\n\\n         for (int i = ...</td>\n",
       "      <td>c-plus-plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53065</td>\n",
       "      <td>if (p-&gt;data &lt; min)\\n\\n         {\\n\\n  ...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               code     language\n",
       "0  14026              var result = testObj1 | testObj2;\\...      c-sharp\n",
       "1  12201          ///     Initializes a new instance of ...      c-sharp\n",
       "2  17074  /*\\n\\n     Explanation :- a user gives a Strin...   javascript\n",
       "3  21102          int sum = 0;\\n\\n         for (int i = ...  c-plus-plus\n",
       "4  53065          if (p->data < min)\\n\\n         {\\n\\n  ...            c"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load and read datasets\n",
    "trainData = pd.read_csv(\"data/train.csv\")\n",
    "testData = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "\n",
    "#use only code and language of dataset and (drop Id column)\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2075e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get id from tokenizer which i use\n",
    "CODEBERTA_LANGUAGE_ID = \"huggingface/CodeBERTa-language-id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ce5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(CODEBERTA_LANGUAGE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "837d5174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 263, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"   \")\n",
    "#ich kann auch einfach meinen eigenen tokenizer bauen indem ich den code nehme und an jedem leerzeichen einen split mache\n",
    "#die frage ist was mÃ¶chte ich haben? \n",
    "#here exists already an vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c774f289",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0n/n106l5rn7cb6yw8038g17vcm0000gn/T/ipykernel_68965/773914927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#vocab = build_vocab_from_iterator(yield_tokens(train_iter[\"code\"]), specials=[\"<unk>\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mordered_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mspecial_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mword_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(ordered_dict, min_freq)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        tokenizer.encode(text)\n",
    "        \n",
    "#was der tokenizer macht ist er nimmt \n",
    "\n",
    "#vocab = build_vocab_from_iterator(yield_tokens(train_iter[\"code\"]), specials=[\"<unk>\"])\n",
    "\n",
    "vocab = build_vocab_from_iterator(trainData[\"code\"].apply(lambda x: tokenizer(x)), specials=[\"<unk>\"])\n",
    "\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1224d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6094ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#converts every word in my code examples from the code column to an integer value\n",
    "vocab=trainData[\"code\"].apply(lambda x: tokenizer.encode(x))\n",
    "#need to find out if vocab which I produce is the same what is pytorch produced\n",
    "#that is wrong\n",
    "#1. create function which loops through every entry from code column\n",
    "#2. check if htat works (with prints etc)\n",
    "#3. create vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
